# CI Acceptance Smoke Test: Non-Gating Topology Validation

**Overview:** The acceptance smoke test is a **best-effort, non-gating** CI job that validates mkolbol topology execution in a realistic scenario. It runs `mkctl` with a FilesystemSink configuration and records pass/fail metrics without blocking PR merges.

---

## Why Acceptance Smoke Tests?

The smoke test answers three critical questions:

1. **Does the topology actually run?** (Not just unit tests passing)
2. **Does data flow correctly?** (FilesystemSink receives and records data)
3. **Is routing working?** (Router endpoints are properly discovered)

These are end-to-end integration checks that **no unit test can fully replicate**. They catch issues like:

- Module instantiation failures in real executor context
- Pipe connection errors between modules
- Heartbeat/health check timeouts
- Router snapshot persistence failures

---

## Smoke Test Architecture

### Three Validation Checks

The acceptance-smoke job runs a single topology and validates three outcomes:

```yaml
Test Configuration:
  â”œâ”€ Module: ExternalProcess (HTTP server)
  â”œâ”€ Module: FilesystemSink (write to JSONL)
  â”œâ”€ Connection: server output â†’ JSONL input
  â””â”€ Duration: 5 seconds

Validation Checks:
  âœ… Check 1: Topology running (grep "Topology running" in logs)
  âœ… Check 2: FilesystemSink JSONL created (file exists with N lines)
  âœ… Check 3: Router endpoints recorded (JSON snapshot with endpoint count)
```

### Non-Gating Design

```
GitHub Actions Workflow:
â”œâ”€ test job: âŒ FAILS â†’ PR BLOCKED (gating, required)
â”œâ”€ consumer-test job: âŒ FAILS â†’ WARNING only (non-gating)
â””â”€ acceptance-smoke job: âŒ FAILS â†’ WARNING only (non-gating)

Benefits:
- Catches real issues without blocking development
- Avoids "flaky test" merge delays
- Aggregates results in PR comments for visibility
- Can be made gating in future when more stable
```

---

## Configuration

### Topology Under Test

File: `examples/configs/http-logs-local-file.yml`

```yaml
nodes:
  - id: web
    module: ExternalProcess
    params:
      command: node
      args:
        - -e
        - "require('http').createServer((req,res)=>{
          console.log(`[${new Date().toISOString()}] ${req.method} ${req.url}`);
          res.end('ok')
          }).listen(3000,()=>console.log('Server listening on http://localhost:3000'))"
      ioMode: stdio
      restart: never

  - id: sink
    module: FilesystemSink
    params:
      filename: reports/http-logs.jsonl

connections:
  - from: web.output
    to: sink.input
```

**Why this config?**

- **Lightweight**: No external processes or complex setup
- **Observable**: Generates JSONL output that can be inspected
- **Isolated**: Doesn't conflict with main unit tests
- **Representative**: Uses real modules (ExternalProcess, FilesystemSink)

### CI Job Configuration

File: `.github/workflows/tests.yml`

```yaml
acceptance-smoke:
  name: Acceptance Smoke Test (FilesystemSink)
  runs-on: ubuntu-latest
  needs: test
  if: ${{ always() }} # Run even if test job fails
  continue-on-error: true # Don't block PR on smoke test failure

  steps:
    - name: Setup
      # ... checkout, npm ci, npm run build ...

    - name: Run acceptance smoke test
      env:
        MK_LOCAL_NODE: '1' # Enforce Local Node mode
      run: |
        timeout 10 node dist/scripts/mkctl.js run \
          --file examples/configs/http-logs-local-file.yml \
          --duration 5 \
          > /tmp/smoke-test.log 2>&1 || true

        # Validation script (see below)
```

---

## Validation Script (CI Implementation)

The smoke test performs three checks and records results to `reports/acceptance-smoke.jsonl`:

### Check 1: Topology Running

```bash
if grep -q "Topology running" /tmp/smoke-test.log; then
  TOPOLOGY_PASS="true"
else
  TOPOLOGY_PASS="false"
fi
```

**What it validates:**

- mkctl started successfully
- Executor initialized nodes without errors
- Topology reached "running" state (not crashed during startup)

**Failure indicators:**

- "Module not found" error
- "Configuration validation failed"
- "Health check failed"
- Process exited with error code

### Check 2: FilesystemSink JSONL Created

```bash
if [ -f reports/http-logs.jsonl ]; then
  FILESINK_PASS="true"
  LOG_LINES=$(wc -l < reports/http-logs.jsonl)
  echo "âœ… FilesystemSink smoke test PASSED: JSONL file created with $LOG_LINES lines"
else
  FILESINK_PASS="false"
  echo "âš ï¸  FilesystemSink smoke test: reports/http-logs.jsonl not found"
fi
```

**What it validates:**

- FilesystemSink module instantiated correctly
- Pipes connected between ExternalProcess â†’ FilesystemSink
- HTTP server emitted data (at least one line logged)
- Data persisted to filesystem

**Failure indicators:**

- File doesn't exist (JSONL never created)
- File empty (pipes not connected or no data emitted)
- File permission error (directory not writable)

### Check 3: Router Endpoints Recorded

```bash
if [ -f reports/router-endpoints.json ]; then
  ENDPOINTS_PASS="true"
  ENDPOINT_COUNT=$(jq 'length' reports/router-endpoints.json)
  echo "âœ… Router endpoints recorded: $ENDPOINT_COUNT endpoints"
else
  ENDPOINTS_PASS="false"
  echo "âš ï¸  Router endpoints not found"
fi
```

**What it validates:**

- RoutingServer (or in-process Router) running
- Both endpoints registered: `web.output` and `sink.input`
- Snapshot persisted to JSON at shutdown
- Router tracked module endpoints correctly

**Failure indicators:**

- File doesn't exist (Router didn't snapshot)
- Endpoint count 0 (modules not registered)
- Endpoint count 1 (one module failed to register)

---

## Results Aggregation

### Recording Results

After all three checks, results are written to `reports/acceptance-smoke.jsonl`:

```json
{
  "type": "acceptance",
  "topology": true,
  "filesink": true,
  "endpoints": true,
  "timestamp": "2025-10-17T04:15:23Z"
}
```

**Format:** One JSON object per line (JSONL), allows appending multiple runs to same file.

### PR Comment Aggregation

File: `scripts/post-laminar-pr-comment.js`

The script reads `reports/acceptance-smoke.jsonl` and generates PR comment section:

```javascript
function readAcceptanceResults() {
  if (!fs.existsSync(ACCEPTANCE_PATH)) return '';

  const lines = fs
    .readFileSync(ACCEPTANCE_PATH, 'utf-8')
    .split('\n')
    .filter((l) => l.trim());

  const result = JSON.parse(lines[lines.length - 1]); // Last result

  const checks = [];
  if (result.topology) checks.push('âœ… Topology');
  else checks.push('âŒ Topology');

  if (result.filesink) checks.push('âœ… FilesystemSink');
  else checks.push('âŒ FilesystemSink');

  if (result.endpoints) checks.push('âœ… Router Endpoints');
  else checks.push('âŒ Router Endpoints');

  return `\n### ðŸ§ª Acceptance Smoke Test\n${checks.join(' | ')}`;
}
```

**Posted to PR as:**

```
### ðŸ§ª Acceptance Smoke Test
âœ… Topology | âœ… FilesystemSink | âœ… Router Endpoints
```

Or if failed:

```
### ðŸ§ª Acceptance Smoke Test
âœ… Topology | âŒ FilesystemSink | âœ… Router Endpoints
```

---

## PR Comment Structure

The aggregated PR comment combines:

1. **Test Summary** (from Laminar unit tests)
2. **Failure Trends** (from Laminar trends history)
3. **Flake Budget** (tests failing â‰¥2 times in last 5 runs)
4. **Acceptance Smoke Test** (three checks: topology, filesink, endpoints)
5. **Artifacts** (links to detailed logs)

```markdown
## ðŸ“Š Laminar Test Report (Aggregated)

### Test Summary

...unit test results...

### Failure Trends

...failure trends...

### ðŸ”´ Flake Budget

...flaky tests...

### ðŸ§ª Acceptance Smoke Test

âœ… Topology | âœ… FilesystemSink | âœ… Router Endpoints

### ðŸ“ Artifacts

- Full Summary: See job artifacts for LAMINAR_SUMMARY.txt
- Repro Hints: See job artifacts for LAMINAR_REPRO.md
- Acceptance Logs: See job artifacts for acceptance-smoke-logs
- Per-Node Reports: Node 20 and Node 24 reports
```

---

## Workflow States and Transitions

### PR Merge Scenarios

```
Scenario 1: All checks pass (Topology + FilesystemSink + Endpoints)
  Acceptance job: âœ… PASS
  PR comment: Shows âœ…âœ…âœ…
  PR merge: ALLOWED (non-gating)
  Developer action: None needed

Scenario 2: FilesystemSink fails (e.g., pipe not connected)
  Acceptance job: âš ï¸ SOFT FAIL
  PR comment: Shows âœ…âŒâœ…
  PR merge: ALLOWED (non-gating, but warning visible)
  Developer action: Review PR comment, investigate if real bug
  Follow-up: May revert PR if investigating shows critical issue

Scenario 3: Unit tests fail (process-mode or threads)
  Test job: âŒ FAILS
  PR: BLOCKED (test job is gating, required)
  Acceptance job: Runs anyway (depends: test, if: always())
  Action: Must fix unit tests first before acceptance smoke runs

Scenario 4: Acceptance smoke times out (topology hangs)
  Acceptance job: âš ï¸ TIMEOUT
  Timeout: 10 seconds (mkctl process terminates via timeout)
  Results: May show partial data (filesink created but incomplete)
  PR comment: Shows mixed results âœ…âŒâœ… or âš ï¸ note
```

---

## Debugging Acceptance Smoke Failures

### If Topology Check Fails

```bash
# Check logs for startup errors
cat /tmp/smoke-test.log | head -20

# Common causes:
# - Module not found: "Unknown module 'ExternalProcess'"
# - Config validation: "Configuration validation failed"
# - Port conflict: "EADDRINUSE: address already in use :::3000"

# Fix: Verify config file is valid
node dist/scripts/mkctl.js run --file examples/configs/http-logs-local-file.yml --dry-run
```

### If FilesystemSink Check Fails

```bash
# Check if directory exists
ls -la reports/

# Check if file created but empty
wc -l reports/http-logs.jsonl

# Check stdout for data emission
cat /tmp/smoke-test.log | grep -i "jsonl\|write"

# Common causes:
# - Pipe not connected: Check YAML connections section
# - Server not emitting: HTTP server crashed before emitting
# - FilesystemSink failed: Check file write permissions

# Fix: Run manually with verbose logging
export MK_LOCAL_NODE=1
node dist/scripts/mkctl.js run --file examples/configs/http-logs-local-file.yml --duration 5 2>&1 | tee /tmp/debug.log
```

### If Router Endpoints Check Fails

```bash
# Check if endpoints snapshot created
ls -la reports/router-endpoints.json

# Count endpoints
jq 'length' reports/router-endpoints.json

# List endpoint names
jq '.[] | .id' reports/router-endpoints.json

# Common causes:
# - Router not initialized: Router requires RoutingServer in config
# - Modules didn't register: Health check failed or module crashed
# - Snapshot not persisted: Topology stopped before snapshot write

# Fix: Verify router is in config (if using in-process routing)
# Local Node mode (MK_LOCAL_NODE=1) uses in-process RoutingServer
```

---

## Manual Testing (Local Reproduction)

To reproduce the acceptance smoke test locally:

```bash
# 1. Build the project
npm run build

# 2. Run mkctl with same config
export MK_LOCAL_NODE=1
timeout 10 node dist/scripts/mkctl.js run \
  --file examples/configs/http-logs-local-file.yml \
  --duration 5

# 3. Check all three artifacts
ls -la reports/
cat reports/http-logs.jsonl
cat reports/router-endpoints.json | jq .

# 4. Simulate CI validation
if [ -f reports/http-logs.jsonl ]; then
  echo "âœ… FilesystemSink pass"
else
  echo "âŒ FilesystemSink fail"
fi

if [ -f reports/router-endpoints.json ]; then
  echo "âœ… Router endpoints pass"
else
  echo "âŒ Router endpoints fail"
fi
```

---

## Integration with Continuous Monitoring

### Flake Budget Calculation

The acceptance smoke results feed into the flake budget (tests failing â‰¥2 times in last 5 runs).

**Current implementation:** Only unit tests tracked. Future enhancement could track acceptance smoke flakiness separately.

### Artifact Persistence

CI artifacts retained for 90 days (GitHub Actions default):

- `/tmp/smoke-test.log` â€” Raw mkctl output
- `reports/acceptance-smoke.jsonl` â€” Result record
- `reports/http-logs.jsonl` â€” Topology output data
- `reports/router-endpoints.json` â€” Endpoint snapshot

**Access in GitHub UI:**

```
Actions â†’ Workflow run â†’ Artifacts â†’ acceptance-smoke-logs
```

---

## Future Enhancements

### Phase 2: Make Smoke Test Gating

```yaml
acceptance-smoke:
  continue-on-error: false # Change to required status

# Would require:
# - Reducing flakiness (network timeouts, timing issues)
# - Adding retries (3 attempts on fail)
# - Pre-warmup (ensure no port conflicts)
```

### Phase 3: Extended Scenarios

```yaml
# Test multiple topology patterns:
# - Split/Merge scenarios
# - Multiple external processes
# - Different I/O modes (PTY, stdio, pipes)
# - Health check edge cases (slow startup, restarts)
```

### Phase 4: Performance Benchmarks

```bash
# Track metrics in JSONL:
# - Startup time (first endpoint registered)
# - Data throughput (bytes/sec through sink)
# - Router latency (endpoint discovery time)
# - Memory usage (peak memory during run)
```

---

---

## MK RC Smoke Test Job

### Overview

The `mk-rc-smoke` job validates the **mk CLI release candidate** workflow: `mk init`, `mk build`, and `mk package`. This is a **non-gating, best-effort** CI job that runs in parallel with the acceptance smoke test.

### Job Configuration

```yaml
mk-rc-smoke:
  name: MK RC Smoke Test (init/build/package)
  runs-on: ubuntu-latest
  needs: test
  if: ${{ always() }} # Run even if test job fails
  continue-on-error: true # Don't block PR on smoke test failure
```

### Test Sequence

The job performs three sequential tests:

1. **mk init test-project** â€” Initialize a new mkolbol project
2. **mk build** â€” Build the initialized project
3. **mk package** â€” Create a distributable tarball

Each test is conditional on the previous step passing.

### Validation Checks

**Check 1: mk init**

```bash
node dist/scripts/mk.js init test-project > /tmp/mk-init.log 2>&1
# Validates: Project scaffolding, package.json creation, default config generation
```

**Check 2: mk build**

```bash
cd test-project
node ../dist/scripts/mk.js build > /tmp/mk-build.log 2>&1
# Validates: TypeScript compilation, module bundling, dist/ output
```

**Check 3: mk package**

```bash
node ../dist/scripts/mk.js package > /tmp/mk-package.log 2>&1
# Validates: Tarball creation, dependency bundling, package metadata
```

### Results Format

Results are recorded to `reports/mk-rc-smoke.jsonl`:

```json
{
  "type": "mk-rc-smoke",
  "init": true,
  "build": true,
  "package": true,
  "timestamp": "2025-10-16T04:15:23Z"
}
```

### Artifacts

CI artifacts include:

- `/tmp/mk-init.log` â€” mk init command output
- `/tmp/mk-build.log` â€” mk build command output
- `/tmp/mk-package.log` â€” mk package command output
- `test-project/` â€” Generated project directory

### Local Reproduction

```bash
npm run build
node dist/scripts/mk.js init test-project
cd test-project && node ../dist/scripts/mk.js build
node ../dist/scripts/mk.js package
ls -la *.tgz
```

---

## MK CI Plan Command

### Overview

The `mk ci plan` command generates CI configuration for GitHub Actions workflows. It outputs test matrices and cache keys in JSON or shell-sourceable format.

### Usage

```bash
# JSON output (default)
mk ci plan

# Shell export format for CI
mk ci plan --env
```

### Output Formats

**JSON (default):**

```json
{
  "matrix": {
    "node": ["20", "24"],
    "lane": ["threads", "forks"]
  },
  "cacheKeys": {
    "node-modules-20": "node-modules-20-abc123",
    "node-modules-24": "node-modules-24-def456"
  }
}
```

**ENV format (--env):**

```bash
export MATRIX_NODE='["20","24"]'
export MATRIX_LANE='["threads","forks"]'
export CACHE_KEY_NODE_MODULES_20=node-modules-20-abc123
export CACHE_KEY_NODE_MODULES_24=node-modules-24-def456
```

### GitHub Actions Integration

```yaml
jobs:
  plan:
    runs-on: ubuntu-latest
    outputs:
      matrix-node: ${{ steps.plan.outputs.matrix-node }}
      matrix-lane: ${{ steps.plan.outputs.matrix-lane }}
    steps:
      - uses: actions/checkout@v4
      - name: Generate CI plan
        id: plan
        run: |
          eval "$(node dist/scripts/mk.js ci plan --env)"
          echo "matrix-node=$MATRIX_NODE" >> $GITHUB_OUTPUT
          echo "matrix-lane=$MATRIX_LANE" >> $GITHUB_OUTPUT

  test:
    needs: plan
    strategy:
      matrix:
        node: ${{ fromJson(needs.plan.outputs.matrix-node) }}
        lane: ${{ fromJson(needs.plan.outputs.matrix-lane) }}
    runs-on: ubuntu-latest
    steps:
      - name: Run tests
        run: npm run test:${{ matrix.lane }}
```

### Local Usage

```bash
# Generate and source environment variables
eval "$(mk ci plan --env)"

# Use in scripts
echo "Node versions: $MATRIX_NODE"
echo "Cache key: $CACHE_KEY_NODE_MODULES_20"
```

---

## TTL Expiry Soak Test (Router Heartbeats)

### Overview

The TTL soak test is a **non-gating, best-effort** test that validates router heartbeat and endpoint expiry behavior under sustained message load. It runs as part of the `mk-acceptance.ts` script and CI acceptance-smoke job.

### Test Architecture

```yaml
Test Configuration:
  â”œâ”€ Duration: 10 seconds sustained load
  â”œâ”€ Message Rate: ~1000 messages/second (100 msgs every 100ms)
  â”œâ”€ Modules: ExternalProcess â†’ PipeMeterTransform â†’ FilesystemSink
  â””â”€ Router: Heartbeat/TTL tracking enabled (MK_LOCAL_NODE=1)

Validation Checks: âœ… Topology runs successfully under load
  âœ… Data throughput sustained (â‰¥10 messages)
  âœ… Router endpoints tracked with heartbeat metadata
  âœ… Stale endpoint detection metadata present
```

### What It Tests

**1. Sustained Load Handling**

- Topology processes ~10,000 messages over 10 seconds
- Verifies no crashes, deadlocks, or dropped connections
- Measures data throughput via FilesystemSink output

**2. Router Heartbeat Mechanism**

- Endpoints registered and tracked in router snapshot
- Heartbeat/TTL metadata recorded for each endpoint
- Validates infrastructure for stale endpoint eviction

**3. TTL Expiry Readiness**

- Confirms presence of `lastHeartbeat` and `ttlMs` fields
- Verifies router can identify stale endpoints (future feature)
- Tests endpoint snapshot persistence under load

### Configuration

The test creates a dynamic topology with high message volume:

```yaml
nodes:
  - id: source
    module: ExternalProcess
    params:
      command: node
      args:
        - -e
        - "setInterval(() => {
          for(let i=0; i<100; i++)
          console.log('msg-' + Date.now() + '-' + i);
          }, 100);"
      ioMode: stdio

  - id: meter
    module: PipeMeterTransform
    params:
      emitInterval: 1000

  - id: sink
    module: FilesystemSink
    params:
      path: reports/ttl-soak.jsonl
```

**Why this config?**

- **High volume**: 100 messages every 100ms = 1000 msg/sec
- **Observable**: JSONL output provides line count for throughput measurement
- **Realistic**: Stresses pipe buffers, router tracking, and file I/O
- **Isolated**: Uses temporary test topology in reports/ directory

### Validation Logic

**Check 1: Topology Started**

```typescript
if (!logOutput.includes('Topology running') && !logOutput.includes('Starting')) {
  throw new Error('Topology did not start successfully');
}
```

**Check 2: Data Throughput**

```typescript
const lineCount = outputContent.split('\n').filter((l) => l.trim()).length;
if (lineCount < 10) {
  throw new Error(`Insufficient data throughput (${lineCount} lines)`);
}
```

**Check 3: Router Heartbeat Metadata**

```typescript
const hasHeartbeatData = endpoints.some(
  (ep: any) => ep.lastHeartbeat !== undefined || ep.ttlMs !== undefined,
);
```

### CI Integration

The TTL soak test runs as part of the `acceptance-smoke` job:

```yaml
acceptance-smoke:
  name: Acceptance Smoke Test (FilesystemSink)
  runs-on: ubuntu-latest
  continue-on-error: true # Non-gating

  steps:
    - name: Run acceptance tests
      run: npm run test:acceptance
      # Includes mk-acceptance.ts which runs TTL soak test
```

Results are aggregated in the acceptance test report:

- `reports/mk-acceptance-results.md` â€” Full test results with TTL soak
- Test duration and throughput metrics logged
- Router endpoint snapshot saved as artifact

### Debugging TTL Soak Failures

**If topology fails to start:**

```bash
# Check build artifacts
npm run build

# Run manually with verbose output
timeout 12 node dist/scripts/mkctl.js run \
  --file reports/ttl-soak-topology.yml \
  --duration 10
```

**If throughput is low:**

```bash
# Check JSONL output
wc -l reports/ttl-soak.jsonl
head -n 20 reports/ttl-soak.jsonl

# Verify ExternalProcess is emitting
# Should see ~1000 messages per second
```

**If router metadata missing:**

```bash
# Check endpoint snapshot
cat reports/router-endpoints.json | jq '.[] | {id, lastHeartbeat, ttlMs}'

# Verify MK_LOCAL_NODE mode enabled
export MK_LOCAL_NODE=1
# (Local Node mode uses in-process RoutingServer with heartbeats)
```

### Local Reproduction

```bash
# 1. Build the project
npm run build

# 2. Run TTL soak test manually
timeout 12 node dist/scripts/mkctl.js run \
  --file reports/ttl-soak-topology.yml \
  --duration 10

# 3. Verify outputs
wc -l reports/ttl-soak.jsonl
cat reports/router-endpoints.json | jq '.'

# 4. Check throughput
echo "Messages processed: $(wc -l < reports/ttl-soak.jsonl)"
echo "Expected: ~10,000 messages (1000/sec * 10 sec)"
```

### Non-Gating Rationale

The TTL soak test is **non-gating** because:

1. **Infrastructure readiness test** â€” Validates heartbeat plumbing exists, not functional eviction
2. **Performance sensitive** â€” Throughput varies by CI runner (CPU, I/O)
3. **Future-proofing** â€” Tests metadata for not-yet-implemented stale endpoint eviction
4. **Best-effort validation** â€” Failure indicates potential issues but doesn't block PRs

When stale endpoint eviction is fully implemented, this test can be extended to:

- Stop sending heartbeats mid-run
- Verify endpoints removed after TTL expires
- Test routing failover when endpoints become stale

---

## Related Documentation

- **[Doctor Guide](./doctor.md)** â€” Troubleshooting mkctl errors and health checks
- **[Acceptance Pack](../../tests/devex/acceptance/local-node-v1.md)** â€” Acceptance test checklist
- **[Laminar Workflow](./laminar-workflow.md)** â€” Test observability and trends
- **[mkctl Cookbook](./mkctl-cookbook.md)** â€” Daily reference for mkctl commands
- **[CI/CD Integration](../../.github/workflows/tests.yml)** â€” Full GitHub Actions workflow

---

## Quick Reference

### For Developers

**"My PR has a smoke test warning, should I be concerned?"**

Check the PR comment:

- âœ…âœ…âœ… All pass? No action needed
- âœ…âŒâœ… One failed? Review your changesâ€”likely a real issue in routing or data flow
- âŒâŒâŒ All failed? Check if dependent test job failed first

**"I want to reproduce this locally"**

```bash
npm run build
export MK_LOCAL_NODE=1
node dist/scripts/mkctl.js run --file examples/configs/http-logs-local-file.yml --duration 5
ls -la reports/
```

### For Maintainers

**"How do I disable smoke tests temporarily?"**

```yaml
# In .github/workflows/tests.yml
acceptance-smoke:
  if: ${{ false }} # Disable job
```

**"I want to change the test topology"**

1. Create new config: `examples/configs/http-logs-local-file-v2.yml`
2. Update workflow: Change `--file examples/configs/http-logs-local-file-v2.yml`
3. Update this documentation: Reflect new checks/validation logic
4. Test locally first: Run manual reproduction steps above

---

**Status:** Acceptance smoke test is **non-gating** and runs best-effort after unit tests pass. Results aggregate into PR comments for visibility without blocking merges.
